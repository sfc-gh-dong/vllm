spec:
  containers:
    - name: vllm-container
      image: <image_repository_url>/vllm
      resources:
        requests:
          nvidia.com/gpu: 1
        limits:
          nvidia.com/gpu: 1
      env:
        MODEL: meta-llama/Llama-2-7b-chat-hf
        HF_TOKEN: <your_hf_token>
        NUM_GPU: 1
        MAX_GPU_MEMORY: 24Gib
      volumeMounts:
        - name: models
          mountPath: /models

  endpoints:
    - name: llama
      port: 8000
      public: true
  volumes:
    - name: models
      source: "@LLM.PUBLIC.models"

  networkPolicyConfig:
    allowInternetEgress: true
